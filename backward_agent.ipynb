{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'game' from '/home/paul/Documents/ETH/RLforGames/WearhouseKeeper/SokobanSolver/game.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import importlib\n",
    "import game\n",
    "importlib.reload(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size=5, output_size=1):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        self.linear3 = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.linear1(state)\n",
    "        state = self.linear2(state)\n",
    "        state = self.linear3(state)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardAgent():\n",
    "    def __init__(self):\n",
    "        self.value_network = ValueNetwork()\n",
    "        self.optimizer = optim.SGD(self.value_network.parameters(), lr=0.01)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        self.discount = 0.98\n",
    "        self.eps = 0.1\n",
    "    \n",
    "    def policy(self, env):\n",
    "        legal_moves = env.legal_moves()\n",
    "        if len(legal_moves) == 0: # Agent is stuck and can't move\n",
    "            return [None, None, None, None, None, None]\n",
    "        \n",
    "        value_moves = []\n",
    "        for char in legal_moves:\n",
    "            state, reward, done = env.step(char, gamma=self.gamma)\n",
    "            state_tensor = torch.tensor(state)\n",
    "            value_moves.append([char, state, state_tensor, self.value_network(state_tensor), reward, done])\n",
    "        return max(value_moves, key=lambda x:x[3])\n",
    "        \n",
    "    def train(self, number_of_episodes, start, end):\n",
    "        wins=0\n",
    "        moves = []\n",
    "        for episode in tqdm(range(number_of_episodes)):\n",
    "            # create new game instance\n",
    "            ind = random.randint(start, end)\n",
    "            env = game.ReverseGame(game.Game(level_id=ind), disable_prints=True)\n",
    "            if episode % 100 == 0:\n",
    "                moves.append(env.player_position)\n",
    "            state, reward, done = env.state(gamma=self.gamma)\n",
    "            # print(f\"state: {state}, reward: {reward}, done: {done}\")\n",
    "            \n",
    "            # do training on game instance\n",
    "            while not done:\n",
    "                # compute value of current state\n",
    "                state_tensor = torch.tensor(state)\n",
    "                value = self.value_network(state_tensor)\n",
    "                \n",
    "                # calculate next action according to policy\n",
    "                [action, next_state, next_state_tensor, next_value, next_reward, next_done] = self.policy(env)\n",
    "                if episode % 100 == 0:\n",
    "                    moves.append(action)\n",
    "                # print(f\"reward from step: {next_reward}\")\n",
    "                \n",
    "                if action is None: # agent is stuck, we are in the same state as before and use these to make the TD(0) updates\n",
    "                    assert(next_value is None and next_done is None and next_reward is None and next_state_tensor is None and next_state is None)\n",
    "                    next_value = value\n",
    "                    next_state_tensor = state\n",
    "                    next_done = 1\n",
    "                    next_reward = reward\n",
    "                    \n",
    "                                    \n",
    "                # TD(0) update: V(s)←V(s)+α(R+γV(s')−V(s))\n",
    "                target = value + self.alpha*(next_reward + self.gamma*next_value.detach()*(1-int(next_done)) - value) # using detach here to exclude prediction from computation graph as is standard in TD(0)\n",
    "                \n",
    "                # calculate loss\n",
    "                loss = self.loss(value, target.detach())\n",
    "                \n",
    "                # optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # make random move in environment\n",
    "                if np.random.rand() < self.eps:\n",
    "                    legal_moves = env.legal_moves()\n",
    "                    if len(legal_moves) == 0: \n",
    "                        break\n",
    "                    else:\n",
    "                        action = random.choice(legal_moves)\n",
    "                        if episode % 100 == 0:\n",
    "                            moves[-1] = action\n",
    "                        state, reward, done = env.play(action, gamma=self.gamma)\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    done = next_done\n",
    "                    reward = next_reward\n",
    "                    if(action != None): # make the game step if there is a possible move, if there is no possible move, game will end\n",
    "                        s, r, d = env.play(action, gamma=self.gamma)\n",
    "                        if not (state == s and done == d and reward == r):\n",
    "                            env.disable_prints=False\n",
    "                            env.print_board()\n",
    "                            print(env.turn)\n",
    "                            print(f\"print form top: {state}, {reward}, {done}\")\n",
    "                            print(f\"print form bot: {s}, {r}, {d}\")\n",
    "                        assert(state == s and done == d and reward == r)\n",
    "                wins+=reward\n",
    "            \n",
    "            # update learning rate\n",
    "            self.alpha = self.alpha * self.discount\n",
    "        print(f\"solved {wins}/{number_of_episodes}\")\n",
    "        file_path = \"run.json\"\n",
    "        json_data = json.dumps(moves)\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(json_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:19<00:00, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solved 0/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# level 0 works now\n",
    "torch.manual_seed(40)\n",
    "random.seed(40)\n",
    "np.random.seed(40)\n",
    "backwardagent = BackwardAgent()\n",
    "backwardagent.train(number_of_episodes=300, start=3, end=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WarehouseKeeper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
