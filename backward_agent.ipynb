{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'game' from '/home/paul/Documents/ETH/RLforGames/WearhouseKeeper/SokobanSolver/game.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import importlib\n",
    "import game\n",
    "importlib.reload(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size=5, output_size=1):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        self.linear3 = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.linear1(state)\n",
    "        state = self.linear2(state)\n",
    "        state = self.linear3(state)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardAgent():\n",
    "    def __init__(self):\n",
    "        self.value_network = ValueNetwork()\n",
    "        self.optimizer = optim.SGD(self.value_network.parameters(), lr=0.01)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        self.discount = 0.98\n",
    "        self.eps = 0.1\n",
    "    \n",
    "    def policy(self, env):\n",
    "        legal_moves = env.legal_moves()\n",
    "        if len(legal_moves) == 0: # Agent is stuck and can't move\n",
    "            return [None, None, None, None, None, None]\n",
    "        \n",
    "        value_moves = []\n",
    "        for char in legal_moves:\n",
    "            state, reward, done = env.step(char, gamma=self.gamma)\n",
    "            state_tensor = torch.tensor(state)\n",
    "            value_moves.append([char, state, state_tensor, self.value_network(state_tensor), reward, done])\n",
    "        return max(value_moves, key=lambda x:x[3])\n",
    "        \n",
    "    def train(self, number_of_episodes, start, end):\n",
    "        wins=0\n",
    "        for episode in tqdm(range(number_of_episodes)):\n",
    "            # create new game instance\n",
    "            ind = random.randint(start, end)\n",
    "            env = game.ReverseGame(game.Game(level_id=ind), disable_prints=True)\n",
    "            state, reward, done = env.state(gamma=self.gamma)\n",
    "            # print(f\"state: {state}, reward: {reward}, done: {done}\")\n",
    "            \n",
    "            # do training on game instance\n",
    "            while not done:\n",
    "                # compute value of current state\n",
    "                state_tensor = torch.tensor(state)\n",
    "                value = self.value_network(state_tensor)\n",
    "                \n",
    "                # calculate next action according to policy\n",
    "                [action, next_state, next_state_tensor, next_value, next_reward, next_done] = self.policy(env)\n",
    "                # print(f\"reward from step: {next_reward}\")\n",
    "                \n",
    "                if action is None: # agent is stuck, we are in the same state as before and use these to make the TD(0) updates\n",
    "                    assert(next_value is None and done is None and reward is None and next_state_tensor is None and next_state is None)\n",
    "                    next_value = value\n",
    "                    next_state_tensor = state\n",
    "                    next_done = 1\n",
    "                    next_reward = reward\n",
    "                    \n",
    "                                    \n",
    "                # TD(0) update: V(s)←V(s)+α(R+γV(s')−V(s))\n",
    "                target = value + self.alpha*(next_reward + self.gamma*next_value.detach()*(1-int(next_done)) - value) # using detach here to exclude prediction from computation graph as is standard in TD(0)\n",
    "                \n",
    "                # calculate loss\n",
    "                loss = self.loss(value, target.detach())\n",
    "                \n",
    "                # optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # make random move in environment\n",
    "                if np.random.rand() < self.eps:\n",
    "                    legal_moves = env.legal_moves()\n",
    "                    if len(legal_moves) == 0: \n",
    "                        break\n",
    "                    else:\n",
    "                        state, reward, done = env.play(random.choice(legal_moves), gamma=self.gamma)\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    done = next_done\n",
    "                    reward = next_reward\n",
    "                    s, r, d = env.play(action, gamma=self.gamma)\n",
    "                    assert(state == s and done == d and reward == r)\n",
    "                wins+=reward\n",
    "            \n",
    "            # update learning rate\n",
    "            self.alpha = self.alpha * self.discount\n",
    "        print(f\"solved {wins}/{number_of_episodes}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m backwardagent \u001b[38;5;241m=\u001b[39m BackwardAgent()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mbackwardagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[244], line 43\u001b[0m, in \u001b[0;36mBackwardAgent.train\u001b[0;34m(self, number_of_episodes, start, end)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(f\"reward from step: {next_reward}\")\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# agent is stuck, we are in the same state as before and use these to make the TD(0) updates\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(next_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m done \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m next_state_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m next_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     next_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     45\u001b[0m     next_state_tensor \u001b[38;5;241m=\u001b[39m state\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# level 0 works now\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "backwardagent = BackwardAgent()\n",
    "backwardagent.train(number_of_episodes=500, start=1, end=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WarehouseKeeper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
